{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "739df786-dc00-4406-9f87-6b04dde97490",
   "metadata": {},
   "source": [
    "# Cloud deployment with AWS SageMaker\n",
    "This notebook showcases how to will **deploy a sentiment analysis model with AWS SageMaker** using serialized pre-trained models. <br> \n",
    "- A serialized sklearn TF-IDF vectorizer is used to compute word embeddings \n",
    "- A serialized sklearn Logistic Regression to predict polarity (positive or negative)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9c8fdd01-2b21-4dd7-803b-418dcd1531ee",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import joblib\n",
    "import scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "06983b8c",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (3.8.1)\n",
      "Requirement already satisfied: click in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from nltk) (1.3.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from nltk) (2023.12.25)\n",
      "Requirement already satisfied: tqdm in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from nltk) (4.66.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f85fc626-16a0-4444-8279-db8f01cf4def",
   "metadata": {},
   "source": [
    "# 1. Retrieve model artifacts\n",
    "\n",
    "We will download the pre-trained models from the `ss_deploy_2024` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5e64a63f-bdf1-4585-96cb-73f8e68b916f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Download pre-trained model and pre-processing function \n",
    "# !wget https://raw.githubusercontent.com/laudavid/ss2024_deploy_app/main/streamlit-app/saved_models/logistic_regression.sav # logistic regression model\n",
    "# !wget https://raw.githubusercontent.com/laudavid/ss2024_deploy_app/main/streamlit-app/saved_models/tfidf-vectorizer.sav # tf-idf vectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6259fe34",
   "metadata": {},
   "source": [
    "# 2. Build an inference script\n",
    "\n",
    "AWS Sagemaker requires an **inference script** to load the pre-trained model and run predictions. <br>\n",
    "This script contains the following elements:\n",
    "-  `model_fn()`: A function that loads the pre-trained models \n",
    "-  `predict_fn()`: A function that generates predictions. It includes steps to clean the input data, apply the tf-idf vectorizer and generate predictions with a logistic regression model.\n",
    "- `input_fn()`/`output_fn()`: Functions for input and output request processing.\n",
    "\n",
    "*Note: You should always use these function names since SageMaker expects these specific functions to exist when deploying models.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "be181958",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting inference.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile inference.py\n",
    "\n",
    "import os\n",
    "import json\n",
    "import joblib \n",
    "from preprocess import text_preprocessing\n",
    "\n",
    "def model_fn(model_dir):\n",
    "    \"\"\"\n",
    "    Load pre-trained models\n",
    "    \"\"\"\n",
    "    model = joblib.load(os.path.join(model_dir, 'logistic_regression.sav'))\n",
    "    tfidf = joblib.load(os.path.join(model_dir, 'tfidf-vectorizer.sav'))\n",
    "    model_dict = {\"vectorizer\":tfidf, \"model\":model}\n",
    "    \n",
    "    return model_dict\n",
    "\n",
    "\n",
    "def predict_fn(input_data, model):\n",
    "    \"\"\"\n",
    "    Apply text vectorizer and model to the incoming request\n",
    "    \"\"\"\n",
    "    tfidf = model['vectorizer']\n",
    "    lr_model = model['model']\n",
    "        \n",
    "    clean_text = text_preprocessing(input_data)\n",
    "    embedding = tfidf.transform(clean_text)\n",
    "    prediction = lr_model.predict(embedding)\n",
    "\n",
    "    return prediction.tolist()\n",
    "\n",
    "\n",
    "def input_fn(request_body, request_content_type):\n",
    "    \"\"\"\n",
    "    Deserialize and prepare the prediction input\n",
    "    \"\"\"\n",
    "\n",
    "    if request_content_type == \"application/json\":\n",
    "        request = json.loads(request_body)\n",
    "    else:\n",
    "        request = request_body\n",
    "\n",
    "    return request\n",
    "\n",
    "\n",
    "def output_fn(prediction, response_content_type):\n",
    "    \"\"\"\n",
    "    Serialize and prepare the prediction output\n",
    "    \"\"\"\n",
    "\n",
    "    if response_content_type == \"application/json\":\n",
    "        response = json.dumps(prediction)\n",
    "    else:\n",
    "        response = str(prediction) \n",
    "\n",
    "    return response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "612bcc3b",
   "metadata": {},
   "source": [
    "# 3. Create a requirements.txt file\n",
    "\n",
    "The `requirements.txt` file allows SageMaker to install the packaged needed for your model to run. <br>\n",
    "In our case, we added `nltk`as the only external library to install.\n",
    "\n",
    "*Note: Some third-party libraries such as scikit-learn or pandas are pre-installed in SageMaker and don't need to be added to this file.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "85069434",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting requirements.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile requirements.txt \n",
    "\n",
    "nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad253353",
   "metadata": {},
   "source": [
    "# 4. Package and upload to S3\n",
    "SageMaker requires that the deployment package be structured in a compatible format. <br> It expects all files to be packaged in a tar archive named **\"model.tar.gz\"** with gzip compression \n",
    "\n",
    "We are going to package the pre-trained models, preprocessing and inferencing scripts, as well as the requirements.txt file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5c934535",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logistic_regression.sav\n",
      "tfidf-vectorizer.sav\n",
      "preprocess.py\n",
      "inference.py\n",
      "requirements.txt\n"
     ]
    }
   ],
   "source": [
    "# Create a tarball with the models, scripts, and requirements\n",
    "!tar -cvpzf model.tar.gz logistic_regression.sav tfidf-vectorizer.sav preprocess.py inference.py requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad24ddc6",
   "metadata": {},
   "source": [
    "Now that the model is packaged, we can upload it to an Amazon S3 bucket. <br>\n",
    "We are going to use `boto3` which is an open-source Python package that allows you to easily interact with other AWS services, in our case Amazon S3 buckets to store the packaged model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "65c42191",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import boto3 # Python package to interact with AWS services (S3,...)\n",
    "import sagemaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a8f7b7b9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get the S3 bucket created when creating this notebook\n",
    "sagemaker_session = sagemaker.Session()\n",
    "bucket = sagemaker_session.default_bucket()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "6268e587",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Save model to the S3 bucket\n",
    "prefix = 'sentiment_analysis_lr'\n",
    "\n",
    "boto3.resource('s3').Bucket(bucket).Object(f'{prefix}/model.tar.gz').upload_file('model.tar.gz')\n",
    "model_data = f's3://{bucket}/{prefix}/model.tar.gz'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c32ebf9-7f98-4561-9b87-fa86f1d0ea0a",
   "metadata": {},
   "source": [
    "# 5. Deploy the model\n",
    "We will now use the **SageMaker Python SDK** package to deploy our model to an API endpoint using the package's scikit-learn frameworks. <br> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f5bb7265-5727-4043-8ac8-dd3f6a3725af",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import time\n",
    "from sagemaker.sklearn.model import SKLearnModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "08388f06",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!sudo chmod 777 lost+found # get the right permissions for the lost+found folder for deployment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72d5c357",
   "metadata": {},
   "source": [
    "To deploy the model, you have to **select an instance type** that is in the same region as your session/bucket's region. <br>\n",
    "To learn more about each region's available instance types, click here: https://aws.amazon.com/fr/sagemaker/pricing/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e132ea4c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eu-west-3\n"
     ]
    }
   ],
   "source": [
    "# Check region of your notebook instance\n",
    "session = boto3.Session()\n",
    "region = session.region_name\n",
    "print(region)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b738581a-86ab-4964-b3b8-e93020693626",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get IAM role created with the notebook\n",
    "role = sagemaker.get_execution_role()\n",
    "\n",
    "# Define name of model endpoint (not mandatory)\n",
    "endpoint_name = \"sentiment-analysis-\" + time.strftime(\"%Y-%m-%d-%H-%M-%S\", time.gmtime())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c010babb",
   "metadata": {},
   "source": [
    "**`SKLearnModel`** allows you to build a SageMaker model for deployment using the packaged model <br>\n",
    "Here are some of its important parameters:\n",
    "- `model_data`: The packaged model's S3 bucket location \n",
    "- `role`: The IAM role to access the S3 bucket \n",
    "- `entry_point`: Path of the inference.py file (the file that is executed as the entry point to model hosting)\n",
    "- `source_dir`: A directory that contains the preprocessing, inference and requirements scripts\n",
    "- `framework_version`: The scikit-learn package version (it should be the same as the serialized models)\n",
    "- `py_version`: Python version to execute the model code\n",
    "\n",
    "For more information, visit SageMaker Python SDK's documentation: https://sagemaker.readthedocs.io/en/stable/frameworks/sklearn/sagemaker.sklearn.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "a2c489c1-6a08-4461-b25b-e8e93b9e5a48",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------!"
     ]
    }
   ],
   "source": [
    "# Build SKLearn SageMaker model\n",
    "sklearn_model = SKLearnModel(\n",
    "    model_data=model_data,\n",
    "    role=role,\n",
    "    entry_point='inference.py',\n",
    "    source_dir='.',\n",
    "    framework_version='1.2-1',\n",
    "    py_version='py3'\n",
    ")\n",
    "\n",
    "# Deploy model to a SageMaker scikit-learn model server\n",
    "predictor = sklearn_model.deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type='ml.m5.xlarge',\n",
    "    endpoint_name=endpoint_name\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b310fb86-73ee-4694-8d00-9f783ad49338",
   "metadata": {},
   "source": [
    "## 6. Test the API endpoint\n",
    "We can know test the deployed model's API endpoint using `.predict()`. <br>\n",
    "We specified a JSON serializer and deserialized since SKLearnModel by default has a Numpy serializer and deserializer (which don't fit with our `output_fn` function)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "1e4387e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['negative']\n"
     ]
    }
   ],
   "source": [
    "# Test the deployed model\n",
    "from sagemaker.serializers import JSONSerializer\n",
    "from sagemaker.deserializers import JSONDeserializer\n",
    "\n",
    "data = {'text': \"This movie is terrible. I hated the actors and the story.\"}\n",
    "\n",
    "predictor.serializer = JSONSerializer()\n",
    "predictor.deserializer = JSONDeserializer()\n",
    "\n",
    "response = predictor.predict(json.dumps(data))\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fb8783b",
   "metadata": {},
   "source": [
    "# 7. Delete the endpoint\n",
    "\n",
    "To avoid unnecessary charges, make sure to delete the model and endpoint. <br> \n",
    "Don't forget also to stop (or delete) the notebook instance as well as delete the s3 bucket folder with the packaged model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f3b405f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up the model and endpoint \n",
    "predictor.delete_model()\n",
    "predictor.delete_endpoint()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
